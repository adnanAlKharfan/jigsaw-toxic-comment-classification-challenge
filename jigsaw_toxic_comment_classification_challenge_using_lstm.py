# -*- coding: utf-8 -*-
"""jigsaw-toxic-comment-classification-challenge-using-lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fjHCdC5F_V3zHufs7iEPj9FjDL0MhYS_
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip ./glove.6B.zip
!pip install kaggle
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge
!unzip ./jigsaw-toxic-comment-classification-challenge.zip
!unzip ./train.csv.zip
!unzip ./test.csv.zip
!unzip ./test_labels.csv.zip

import sys
import os
import keras
import sklearn
import numpy as np
import pandas as pd

df_train=pd.read_csv("./train.csv")

target_label=df_train.iloc[:,2:].values

comment_text=df_train["comment_text"].fillna("DUMMY_VALUE").values

word2vec={}
f=open("./glove.6B.100d.txt")    
for line in f:
        temp=line.split()
        word=temp[0]
        vec=np.asarray(temp[1:],dtype="float32")
        word2vec[word]=vec

tokenizer=keras.preprocessing.text.Tokenizer(num_words=20000)
tokenizer.fit_on_texts(comment_text)
sequance=tokenizer.texts_to_sequences(comment_text)
word2index=tokenizer.word_index

max_sequance_len=max(len(s) for s in sequance)

sequance=keras.preprocessing.sequence.pad_sequences(sequance,maxlen=max_sequance_len)

embedding_matrix=np.zeros((len(word2index.keys()),100))

for word,i in word2index.items():
    if i<len(word2index.keys()):
        embedding_vec=word2vec.get(word)
        if  embedding_vec is not None:
            embedding_matrix[i]=embedding_vec

embedding_layer=keras.layers.Embedding(len(word2index.keys()),100,weights=[embedding_matrix],trainable=False
                                        ,input_length=max_sequance_len)

input_=keras.layers.Input(shape=(max_sequance_len,))
x=embedding_layer(input_)
x=keras.layers.LSTM(128,return_sequences=True,dropout=0.1)(x)
x=keras.layers.LSTM(128,return_sequences=True,dropout=0.1)(x)
x=keras.layers.LSTM(128,return_sequences=True,dropout=0.1)(x)
x=keras.layers.GlobalMaxPool1D()(x)
y=keras.layers.Dense(6,activation="sigmoid")(x)
model=keras.Model(input_,y)

model.compile(optimizer="rmsprop",
              loss=keras.losses.BinaryCrossentropy(),
              metrics=["accuracy"])

model.fit(sequance,target_label,epochs=10,batch_size=32, validation_split=0.2)

df_test=pd.read_csv("./test.csv")

target_label_test=pd.read_csv("./test_labels.csv")

comment_text_test=df_test["comment_text"].fillna("DUMMY_VALUE").values

sequance_test=tokenizer.texts_to_sequences(comment_text_test)

sequance_test=keras.preprocessing.sequence.pad_sequences(sequance_test,maxlen=max_sequance_len)

model.evaluate(sequance_test,target_label_test.iloc[:,1:].values)

sequance_try=tokenizer.texts_to_sequences(["fuck you"])
sequance_try=keras.preprocessing.sequence.pad_sequences(sequance_try,maxlen=max_sequance_len)

model.predict(sequance_try)

df_train.head()

